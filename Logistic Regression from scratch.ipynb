{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** Umberto Michelucci, (C) 2017\n",
    "\n",
    "In this review I do a complete discussion of logistic regression for binary classification. I perform all the mathematical derivations, including backpropagation and optimisation. I derive the necessary equations for one training cases, and then I derive the ones for $m$ training cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose we want to be able to classify some observations in two classes. For example we may want to know if there is a cat in an image or not. Or we may want to know if a person will change insurance in the next six months or not.\n",
    "Basically we will have some features that describe what we are observing (for example the RGB values of the pixel of the image, or the age, address, weight of a person) and we want to be able to predict if the observation is of class one or two (cat or no-cat, change insurance or not). This kind of classification is called binary classification.\n",
    "Logistic regression, in its easier form, is used to perform exactly this. \n",
    "\n",
    "Our goal is to be able to have a model that we can train (let it learn from examples) and that can predict if a certain new observation (its input) is of one of two classes.\n",
    "\n",
    "Let's first clarify some notation that we will use in this paper.\n",
    "\n",
    "Our prediction will be a variable $a$ that can only be $0$ or $1$ (we will indicate with $0$ and $1$ the two classes we are trying to predict). In a more mathematical form we will have\n",
    "$$\n",
    "\\text{prediction / estimate}\\ \\ \\rightarrow \\ \\ a\n",
    "\\in \\{0,1\\}\n",
    "$$\n",
    "\n",
    "But what our method will give as output, or as a prediction, will be the probability of $a$ being 1, given the input case $x$. Or in a more mathematical form:\n",
    "\n",
    "$$\n",
    "a = P(y = 1 \\ |\\ x)\n",
    "$$\n",
    "\n",
    "usually we will then define an input observation to be of class $1$ if $a>0.5$ and of classe $0$ if $a <= 0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we have $n_x$ input features (let's assume they are numerical): $x_1, x_2, ..., x_{n_x}$. We will indicate the vector with $x$ \n",
    "\n",
    "$$\n",
    "x = (x_1, x_2, ..., x_{n_x})\n",
    "$$\n",
    "\n",
    "In our discussion we will also use the vector $w$ that will contain $n_x$ weigths (also numerical) and a constant $b$ (number) (called bias):\n",
    "\n",
    "$$\n",
    "w = (w_1, w_2, ..., w_{n_x})\n",
    "$$\n",
    "\n",
    "\n",
    "As you may know, what we need to do is to find the ideal weights $w$ and bias $b$ to classify our observations in the best way possible (what exactly means will be discussed later)\n",
    "\n",
    "Let's suppose for a moment that we have already found the ideal $w$ and $b$.\n",
    "To apply the method we will then have to perform the following steps\n",
    "\n",
    "### Step 1\n",
    "\n",
    "We first build a linear combination $z$ of our inputs using $w$ and $b$\n",
    "\n",
    "$$\n",
    "z = w_1 x_1+w_2 x_2+...+w_{n_x} x_{n_x}+b\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "Now it will be very useful to consider our vectors $x$ or $w$ as tensors (or matrices) of dimensions $(n_x, 1)$. This will make the generalisation to many training cases a lot easier, since we will be able to use the formula we will find (and that we will need to vectorize, more on that later) with minor modifications. So we have $x$ and $w$ and both have the dimensions $(n_x,1)$. Equation (1) can then be rewritten as a matrix multiplication.\n",
    "\n",
    "$$\n",
    "z = w^T x + b = w_1 x_1+w_2 x_2+...+w_{n_x} x_{n_x}+b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "We will then use the sigmoid function applied to $z$\n",
    "\n",
    "$$\n",
    "a = \\sigma (z) = \\sigma (w^T x + b)\n",
    "$$\n",
    "\n",
    "where with $w^T$ we have indicated the transpose of the vector $w$ (as already explained above). Note that this can have values only between 0 and 1, so it behaves as a probability.\n",
    "\n",
    "Now we have assumed to have already found the ideal weights $w$ and bias $b$ so that the $a$ we get from the previous equation is already our prediction. We don't need to do anything else. But the whole idea of Machine Learning is, well, learning. So that means that we need to find the ideal $w$ and $b$ given a set of features (the $x$ in our notation).\n",
    "But how to learn? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "To find the best parameters $w$ and $b$, we will need to minimize what is called the cost function (CF). A complete discussion about it is outside the scope of this paper. The CF we will use is the cross entropy (to be precise the following equation is for the Loss function, but as will be clear later on, for one observation the Loss and the Cost function are the same).\n",
    "\n",
    "$$\n",
    "\\mathscr{L} (a,y) = -\\left[ y \\log a + (1-y) \\log (1-a) \\right]\n",
    "$$\n",
    "\n",
    "It is useful to draw a computational graph to show the steps necessary to calculate $\\mathscr{L} (a,y)$.\n",
    "Please note that we are still discussing a single training observation. We will generalize the approach in the next section.\n",
    "\n",
    "So to reiterate the idea we will need to find the parameters $w$ and $b$ that minimize $\\mathscr{L} (a,y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computational graph to calculate $\\mathscr{L} (a,y)$ is the following\n",
    "\n",
    "![title](logistic_regression_computational_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimize $\\mathscr{L} (a,y)$ we will use the gradient descent algorithm. For a theoretical discussion of the method (well worth knowing) you can refer to several books (or to get an idea from Wikipedia: https://en.wikipedia.org/wiki/Gradient_descent)\n",
    "\n",
    "So to be able to use the algorithm we will need to have all the partial derivatives of $\\mathscr{L} (a,y)$ with respect to the weights $w_j$ (with $j \\in {1,..., n_x})$ and $b$. So we will need to calculate\n",
    "\n",
    "$$\\displaystyle\n",
    "\\frac{\\partial \\mathscr{L} (a,y)}{\\partial w_j}\n",
    "\\tag{2}\n",
    "$$\n",
    "and\n",
    "$$\\displaystyle\n",
    "\\frac{\\partial \\mathscr{L} (a,y)}{\\partial b}\n",
    "\\tag{3}\n",
    "$$\n",
    "\n",
    "then during the \"descent\", at step $n+1$ we will need to update the parameters from step $n$. \n",
    "Let's indicate the parameter $w_j$ at step $n$ with the notation $w_{j,[n]}$ and $b$ with $b_{[n]}$.\n",
    "We can then use the formulas\n",
    "\n",
    "$$\\displaystyle\n",
    "w_{j, [n+1]} = w_{j,[n]} -\\alpha \\frac{\\partial \\mathscr{L} (a,y)}{\\partial w_j} \\ \\ \\ \\ j \\in \\{1,2, ..., n_x\\}\n",
    "\\tag{3a}\n",
    "$$\n",
    "and\n",
    "$$\\displaystyle\n",
    "b_{[n+1]} = b_{[n]} -\\alpha \\frac{\\partial \\mathscr{L} (a,y)}{\\partial b}\n",
    "\\tag{3b}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is called the learning rate and is an additional parameter that needs to be optimized. But for the moment we will consider it as a constant.\n",
    "\n",
    "So if we want to implement the entire algorithm from scratch we will need explicit equations for the partial derivatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To calculate the derivatives in equation (2) and (3) we can simply apply the chain rule (for those who knows calculus). $\\mathscr{L} (a,y)$ is simply the composition of several function. So we can easily write (try to derive it yourself)\n",
    "\n",
    "$$\\displaystyle\n",
    "\\frac{\\partial \\mathscr{L} (a,y)}{\\partial w_j} = \\frac{\\partial \\mathscr{L} (a,y)}{\\partial a} \\frac{da}{dz}\\frac{\\partial z}{\\partial w_j} \\ \\ \\ \\ j \\in \\{1,2, ..., n_x\\}\n",
    "\\tag{4}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\displaystyle\n",
    "\\frac{\\partial \\mathscr{L} (a,y)}{\\partial b} = \\frac{\\partial \\mathscr{L} (a,y)}{\\partial a} \\frac{da}{dz}\\frac{\\partial z}{\\partial b}\n",
    "\\tag{5}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily calculate all the derivates appearing in (4) and (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\displaystyle\n",
    "\\frac{\\partial \\mathscr{L} (a,y)}{\\partial a} = - \\frac{y}{a}+ \\frac{1-y}{1-a}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\displaystyle\n",
    "\\frac{da}{dz} = a(1-a)\n",
    "$$\n",
    "\n",
    "and finally\n",
    "\n",
    "$$\\displaystyle\n",
    "\\frac{\\partial z}{\\partial w_j} = x_j\n",
    "$$\n",
    "\n",
    "$$\\displaystyle\n",
    "\\frac{\\partial z}{\\partial b} = 1\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we put together those equations with (4) and (5) we get\n",
    "\n",
    "$$\\displaystyle\n",
    "\\frac{\\partial \\mathscr{L} (a,y)}{\\partial w_j} = \\frac{\\partial \\mathscr{L} (a,y)}{\\partial a} \\frac{da}{dz}\\frac{\\partial z}{\\partial w_j} = (a-y) \\ x_j\n",
    "\\tag{6}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\displaystyle\n",
    "\\frac{\\partial \\mathscr{L} (a,y)}{\\partial b} = \\frac{\\partial \\mathscr{L} (a,y)}{\\partial a} \\frac{da}{dz}\\frac{\\partial z}{\\partial b} = (a-y)\n",
    "\\tag{7}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that (3a) and (3b) can be written easily as\n",
    "\n",
    "$$\\displaystyle\n",
    "w_{j, [n+1]} = w_{j,[n]} -\\alpha \\frac{\\partial \\mathscr{L} (a,y)}{\\partial w_j} = w_{j,n} - \\alpha \\ (a-y) \\ x_j\n",
    "\\tag{8}\n",
    "$$\n",
    "and\n",
    "$$\\displaystyle\n",
    "b_{[n+1]} = b_{[n]} -\\alpha \\frac{\\partial \\mathscr{L} (a,y)}{\\partial b} = b_n - \\alpha \\ (a-y)\n",
    "\\tag{9}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation (8) and (9) will be the ones we will use in our gradient descent implementation. If you have heard about backpropagation and computational graphs the following graph will get you immediately equations (5) and (6) (at least the first part, to obtain the $(a-y)x_j$ and $(a-y)$ you still need to perform some derivatives). Remember that in backpropagation you go from right to left, instead that from left to right. Each unit is divided in two halfs. The right one is used in the forward propagation and the left one in the backpropagation. This method is widely used in neural networks, and especially in deep learning. Since logistic regression can be obtained by a neural network with a single sigmoid unit we can use this method to get the right equations for the gradient descent.\n",
    "\n",
    "![Computational Graph for the backpropagation algorithm](logistic_regression_computational_graph2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalisation to many training cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to generalise our equation to the case when we have many training cases. This will be easier than expected. First some notation. Let's indicate with $x^{(i)}$ our $i$th training case. Remember $x^{(i)}$ have the dimension $(n_x,1)$. So they are basically \"vertical\" vectors. Or better formulated $x^{(i)}$ is a $\\mathbb{R}^{n_x \\times 1}$ matrix. Let's indicate with $X$ our matrix containing all the features and all the training cases. So $X$ will be a matrix of dimensions $\\mathbb{R}^{n_x \\times m}$ where with $m$ we have indicated the number of training cases that we have. $X$ can be obtained stacking vertically all the $x^{(i)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's rewrite our equations in matrix form:\n",
    "\n",
    "$$\n",
    "Z = w^T X + B = (\n",
    "\\begin{matrix}\n",
    "w^T x^{(1)}+b & w^T x^{(2)}+b & ... & w^T x^{(m)}+b  \\\\\n",
    "\\end{matrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "where we have indicated with $B$ a matrix of dimension $(1,n_x)$ with all elements equal to $b$. In Python we will not need to define this additional vector since **broadcasting** will take care of it.\n",
    "Note that $Z$ will have the dimensions $(1, m)$, since $w^T$ has dimensions $(1,n_x)$ and $X$ has $(n_x,m)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will need to calculate $A$. This will be a matrix of dimensions $(1,m)$ that will be\n",
    "\n",
    "$$\n",
    "A = \\sigma (Z) = [\n",
    "\\begin{matrix}\n",
    "\\sigma (w^T x^{(1)}+b) &\\sigma (w^T x^{(2)}+b) & ... & \\sigma (w^T x^{(m)}+b)  \\\\\n",
    "\\end{matrix}\n",
    "]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I mentioned before that the Loss function and the cost function for one training case are the same. When we are dealing with many training cases we will define our cost function as the average of our loss function calculated over all training cases as:\n",
    "\n",
    "$$\\displaystyle\n",
    "J(w,b) = \\frac{1}{m} \\sum_{i=1}^{m} \\mathscr{L} (a^{(i)},y^{(i)})\n",
    "$$\n",
    "\n",
    "Now we will need to calculate the derivative of our new cost function to implement the backpropagation algorithm to determine our parameters. This will not be difficult, since the derivative of the sum of functions is the sum of derivatives so we have simply\n",
    "\n",
    "$$\\displaystyle\n",
    "\\frac{\\partial J(w,b)}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} \n",
    "\\frac{\\partial J(w,b)}{\\partial a^{(i)}} \\frac{da^{(i)}}{dz^{(i)}}\\frac{\\partial z^{(i)}}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m}(a^{(i)}-y^{(i)}) \\ x^{(i)}_j\n",
    "\\tag{8}\n",
    "$$\n",
    "\n",
    "and for $b$\n",
    "\n",
    "$$\\displaystyle\n",
    "\\frac{\\partial J(w,b)}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} \n",
    "\\frac{\\partial J(w,b)}{\\partial a^{(i)}} \\frac{da^{(i)}}{dz^{(i)}}\\frac{\\partial z^{(i)}}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m}(a^{(i)}-y^{(i)}) \n",
    "\\tag{9}\n",
    "$$\n",
    "\n",
    "Now we can define the following matrix\n",
    "$$\\displaystyle\n",
    "A = \\left[\n",
    "\\begin{matrix}\n",
    "a^{(1)}& a^{(2)} & ... & a^{(m)}  \\\\\n",
    "\\end{matrix}\n",
    "\\right] \\ \\ \\ \\in \\mathbb{R}^{1 \\times m}\n",
    "$$\n",
    "\n",
    "and we will have\n",
    "\n",
    "$$\n",
    "A-Y = \\left[\n",
    "\\begin{matrix}\n",
    "a^{(1)}-y^{(1)}& a^{(2)}-y^{(2)} & ... & a^{(m)}-y^{(m)}  \\\\\n",
    "\\end{matrix}\n",
    "\\right] \\ \\ \\ \\in \\mathbb{R}^{1 \\times m}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first will need to evaluate another matrix (since we have many parameters). So we will need to find (remember that w is not a scalar, and that has dimensions $(n_x,1)$)\n",
    "\n",
    "$$\n",
    "\\nabla_w J(w,b) = \\frac{1}{m} X^T (A-Y)\n",
    "\\tag{10}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(w,b)}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m}(A_i-Y_i) \n",
    "\\tag{11}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with the equation (10) and (11) you can update the parameters to implement the gradient descent algorithm. So we can write the value of $w$ at step $n+1$ as\n",
    "\n",
    "$$\n",
    "w_{[n+1]} = w_{[n]}-\\alpha \\frac{1}{m} X^T (A-Y)\n",
    "$$\n",
    "\n",
    "and for $b$\n",
    "\n",
    "$$\n",
    "b_{[n+1]} = b_{[n]}-\\alpha \\frac{1}{m} \\sum_{i=1}^{m}(A_i-Y_i) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be easily implemented in Python giving a general method to evaluate and find the optimal parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
